{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPapLR816yDDdJjbrDpaz2z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/canerskrc/Deep_Learning_Project/blob/main/GPT_Next_Word_Prediction_with_LSTM_in_Python_A_Dynamic_Text_Completion_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "prJ5deQYJj1O",
        "outputId": "d0fa0553-3ec5-4898-d291-98a2bcfd680e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3eb8520887a7>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRMSprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_shell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessagebox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshowerror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mShowText\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/draw/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nltk.draw package not loaded (please install Tkinter library).\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProductionList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCFGEditor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCFGDemo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     from nltk.draw.tree import (\n\u001b[1;32m     19\u001b[0m         \u001b[0mTreeSegmentWidget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/draw/cfg.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m )\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTreeSegmentWidget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_to_treesegment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m from nltk.draw.util import (\n\u001b[1;32m     65\u001b[0m     \u001b[0mCanvasFrame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/draw/tree.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtkinter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIntVar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMenu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m from nltk.draw.util import (\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mBoxWidget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mCanvasFrame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM, Dense, Activation\n",
        "from keras.optimizers import RMSprop\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import heapq\n",
        "\n",
        "# 1. Veri Yükleme ve Tokenizasyon\n",
        "def load_and_tokenize_data(file_path):\n",
        "    \"\"\"\n",
        "    Metin dosyasını yükler ve kelimelere böler.\n",
        "\n",
        "    Args:\n",
        "    file_path (str): Yüklenecek dosya yolu.\n",
        "\n",
        "    Returns:\n",
        "    list: Tokenize edilmiş kelimeler listesi.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            text = file.read().lower()\n",
        "        print(f'Corpus length: {len(text)}')\n",
        "        tokenizer = RegexpTokenizer(r'\\w+')\n",
        "        words = tokenizer.tokenize(text)\n",
        "        return words\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Dosya {file_path} bulunamadı.\")\n",
        "        return []\n",
        "\n",
        "# 2. Benzersiz Kelimeleri Bulma ve İndeksleme\n",
        "def create_unique_word_index(words):\n",
        "    \"\"\"\n",
        "    Benzersiz kelimeler için indeks oluşturur.\n",
        "\n",
        "    Args:\n",
        "    words (list): Tokenize edilmiş kelimeler listesi.\n",
        "\n",
        "    Returns:\n",
        "    dict: Kelime indeksleri, numpy array: Benzersiz kelimeler\n",
        "    \"\"\"\n",
        "    unique_words = np.unique(words)\n",
        "    unique_word_index = {word: i for i, word in enumerate(unique_words)}\n",
        "    return unique_word_index, unique_words\n",
        "\n",
        "# 3. Eğitim ve Hedef Verilerinin Hazırlanması\n",
        "def prepare_training_data(words, unique_word_index, word_length):\n",
        "    \"\"\"\n",
        "    Eğitim için giriş ve çıkış verilerini hazırlar (one-hot encoding).\n",
        "\n",
        "    Args:\n",
        "    words (list): Tokenize edilmiş kelimeler listesi.\n",
        "    unique_word_index (dict): Benzersiz kelimelerin indeksleri.\n",
        "    word_length (int): Her bir örnekteki kelime sayısı (n-gram).\n",
        "\n",
        "    Returns:\n",
        "    np.array: Eğitim verisi (X), np.array: Hedef verisi (Y)\n",
        "    \"\"\"\n",
        "    prev_words = []\n",
        "    next_words = []\n",
        "\n",
        "    for i in range(len(words) - word_length):\n",
        "        prev_words.append(words[i:i + word_length])\n",
        "        next_words.append(words[i + word_length])\n",
        "\n",
        "    X = np.zeros((len(prev_words), word_length, len(unique_word_index)), dtype=bool)\n",
        "    Y = np.zeros((len(next_words), len(unique_word_index)), dtype=bool)\n",
        "\n",
        "    for i, each_words in enumerate(prev_words):\n",
        "        for j, each_word in enumerate(each_words):\n",
        "            X[i, j, unique_word_index[each_word]] = 1\n",
        "        Y[i, unique_word_index[next_words[i]]] = 1\n",
        "\n",
        "    return X, Y\n",
        "\n",
        "# 4. Modelin Oluşturulması\n",
        "def build_model(input_shape, vocab_size):\n",
        "    \"\"\"\n",
        "    LSTM modeli oluşturur.\n",
        "\n",
        "    Args:\n",
        "    input_shape (tuple): Giriş verisinin şekli (word_length, vocab_size).\n",
        "    vocab_size (int): Benzersiz kelime sayısı.\n",
        "\n",
        "    Returns:\n",
        "    Sequential: Derlenmiş LSTM modeli.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(128, input_shape=input_shape))\n",
        "    model.add(Dense(vocab_size))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    optimizer = RMSprop(learning_rate=0.01)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# 5. Modelin Eğitilmesi ve Kaydedilmesi\n",
        "def train_and_save_model(model, X, Y, batch_size=128, epochs=20, validation_split=0.05):\n",
        "    \"\"\"\n",
        "    Modeli eğitir ve kaydeder.\n",
        "\n",
        "    Args:\n",
        "    model (Sequential): LSTM modeli.\n",
        "    X (np.array): Eğitim verisi.\n",
        "    Y (np.array): Hedef verisi.\n",
        "    batch_size (int): Eğitimde kullanılacak batch boyutu.\n",
        "    epochs (int): Eğitim döngü sayısı.\n",
        "    validation_split (float): Validation verisi oranı.\n",
        "    \"\"\"\n",
        "    history = model.fit(X, Y, validation_split=validation_split, batch_size=batch_size, epochs=epochs, shuffle=True)\n",
        "    model.save('keras_next_word_model.h5')\n",
        "    with open(\"history.p\", \"wb\") as f:\n",
        "        pickle.dump(history.history, f)\n",
        "\n",
        "# 6. Modeli Yükleme ve Geçmişi Görselleştirme\n",
        "def load_and_plot_history():\n",
        "    \"\"\"\n",
        "    Eğitilen modeli ve geçmişi yükler, ardından doğruluk ve kayıp grafiği çizer.\n",
        "    \"\"\"\n",
        "    model = load_model('keras_next_word_model.h5')\n",
        "    history = pickle.load(open(\"history.p\", \"rb\"))\n",
        "\n",
        "    plt.plot(history['accuracy'], label='train accuracy')\n",
        "    plt.plot(history['val_accuracy'], label='validation accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(history['loss'], label='train loss')\n",
        "    plt.plot(history['val_loss'], label='validation loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# 7. Metin Tahmini\n",
        "def sample(preds, top_n=3):\n",
        "    \"\"\"\n",
        "    En yüksek olasılığa sahip tahmin edilen kelimeleri seçer.\n",
        "\n",
        "    Args:\n",
        "    preds (np.array): Tahmin edilen olasılıklar.\n",
        "    top_n (int): Döndürülecek en iyi tahmin sayısı.\n",
        "\n",
        "    Returns:\n",
        "    list: En yüksek olasılığa sahip tahminlerin indeksleri.\n",
        "    \"\"\"\n",
        "    preds = np.log(preds + 1e-10)  # Olasılıklarda sıfır olan değerlerden kaçınmak için\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    return heapq.nlargest(top_n, range(len(preds)), preds.take)\n",
        "\n",
        "def predict_completions(text, model, unique_word_index, reverse_word_index, n=3):\n",
        "    \"\"\"\n",
        "    Girilen metin için olası tamamlamaları tahmin eder.\n",
        "\n",
        "    Args:\n",
        "    text (str): Girdi metni.\n",
        "    model (Sequential): Eğitimli LSTM modeli.\n",
        "    unique_word_index (dict): Benzersiz kelime indeksleri.\n",
        "    reverse_word_index (dict): İndekslerden kelimelere dönüşüm sözlüğü.\n",
        "    n (int): Döndürülecek tahmin sayısı.\n",
        "\n",
        "    Returns:\n",
        "    list: Tahmin edilen kelime tamamlamaları.\n",
        "    \"\"\"\n",
        "    def prepare_input(text):\n",
        "        x = np.zeros((1, WORD_LENGTH, len(unique_word_index)))\n",
        "        for t, word in enumerate(text.split()):\n",
        "            if word in unique_word_index:\n",
        "                x[0, t, unique_word_index[word]] = 1\n",
        "        return x\n",
        "\n",
        "    x = prepare_input(text.lower())\n",
        "    preds = model.predict(x, verbose=0)[0]\n",
        "    next_indices = sample(preds, n)\n",
        "    return [reverse_word_index[idx] for idx in next_indices]\n",
        "\n",
        "# 8. Tüm Sürecin Çalıştırılması\n",
        "if __name__ == \"__main__\":\n",
        "    WORD_LENGTH = 5  # Bir cümledeki kelime uzunluğu\n",
        "    path = '1661-0.txt'  # Metin dosyasının yolu\n",
        "\n",
        "    # Veriyi yükle ve işle\n",
        "    words = load_and_tokenize_data(path)\n",
        "    if not words:\n",
        "        exit()\n",
        "\n",
        "    unique_word_index, unique_words = create_unique_word_index(words)\n",
        "    reverse_word_index = {i: word for word, i in unique_word_index.items()}\n",
        "    X, Y = prepare_training_data(words, unique_word_index, WORD_LENGTH)\n",
        "\n",
        "    # Modeli oluştur ve eğit\n",
        "    model = build_model(input_shape=(WORD_LENGTH, len(unique_words)), vocab_size=len(unique_words))\n",
        "    train_and_save_model(model, X, Y, batch_size=128, epochs=20)\n",
        "\n",
        "    # Model geçmişini çiz ve tahmin yap\n",
        "    load_and_plot_history()\n",
        "\n",
        "    # Örnek metin tamamlama\n",
        "    test_text = \"it is not a lack\"\n",
        "    completions = predict_completions(test_text, model, unique_word_index, reverse_word_index, n=3)\n",
        "    print(f'Girilen metin için olası tamamlamalar: {completions}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wwNoU1IMNFWU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}