{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNg82nP+HREE6az5yYDnbwm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/canerskrc/Deep_Learning_Project/blob/main/dog_breed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfMGMTFh3gnD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "from torch import nn, optim\n",
        "from torch.functional import F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['lines.linewidth'] = 2\n",
        "plt.rcParams['font.sans-serif'] = 'Arial'\n",
        "plt.rcParams['text.color'] = 'black'\n",
        "plt.rcParams['axes.labelcolor']= 'black'\n",
        "plt.rcParams['xtick.color'] = 'black'\n",
        "plt.rcParams['ytick.color'] = 'black'\n",
        "plt.rcParams['font.size']=12\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "data_dir = './input/dog-breed-identification/'\n",
        "labels = pd.read_csv(os.path.join(data_dir, 'labels.csv'))\n",
        "assert(len(os.listdir(os.path.join(data_dir, 'train'))) == len(labels))\n",
        "\n",
        "print(labels.head())\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "labels.breed = le.fit_transform(labels.breed)\n",
        "labels.head()\n",
        "\n",
        "print(labels.head())\n",
        "\n",
        "X = labels.id\n",
        "y = labels.breed\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y,test_size=0.4, random_state=SEED, stratify=y)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid, test_size=0.5, random_state=SEED, stratify=y_valid)\n",
        "\n",
        "class Dataset_Interpreter(Dataset):\n",
        "    def __init__(self, data_path, file_names, labels=None, transforms=None):\n",
        "        self.data_path = data_path\n",
        "        self.file_names = file_names\n",
        "        self.labels = labels\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.file_names))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = f'{self.file_names.iloc[idx]}.jpg'\n",
        "        full_address = os.path.join(self.data_path, img_name)\n",
        "        image = Image.open(full_address)\n",
        "        label = self.labels.iloc[idx]\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            image = self.transforms(image)\n",
        "\n",
        "        return np.array(image), label\n",
        "\n",
        "def plot_images(images):\n",
        "\n",
        "    n_images = len(images)\n",
        "\n",
        "    rows = int(np.sqrt(n_images))\n",
        "    cols = int(np.sqrt(n_images))\n",
        "\n",
        "    fig = plt.figure(figsize=(20,10))\n",
        "    for i in range(rows*cols):\n",
        "        ax = fig.add_subplot(rows, cols, i+1)\n",
        "        ax.set_title(f'{le.inverse_transform([images[i][1]])}')\n",
        "        ax.imshow(np.array(images[i][0]))\n",
        "        ax.axis('off')\n",
        "\n",
        "N_IMAGES = 9\n",
        "\n",
        "train_data = Dataset_Interpreter(data_path=data_dir+'train/', file_names=X_train, labels=y_train)\n",
        "images = [(image, label) for image, label in [train_data[i] for i in range(N_IMAGES)]]\n",
        "plot_images(images)\n",
        "\n",
        "normalize = transforms.Normalize(\n",
        "   mean=[0.485, 0.456, 0.406],\n",
        "   std=[0.229, 0.224, 0.225]\n",
        ")\n",
        "transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)\n",
        "train_transforms = transforms.Compose([transforms.Resize(32),\n",
        "                               transforms.CenterCrop(32),\n",
        "                               transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0),\n",
        "                               transforms.RandomHorizontalFlip(p=0.5),\n",
        "                               transforms.RandomVerticalFlip(p=0.5),\n",
        "                               transforms.RandomGrayscale(p=0.1),\n",
        "                               transforms.ToTensor(),\n",
        "                               normalize])\n",
        "test_transforms = transforms.Compose([transforms.Resize(32),\n",
        "                               transforms.CenterCrop(32),\n",
        "                               transforms.ToTensor(),\n",
        "                               normalize])\n",
        "\n",
        "train_data = Dataset_Interpreter(data_path=data_dir+'train/', file_names=X_train, labels=y_train, transforms=train_transforms)\n",
        "valid_data = Dataset_Interpreter(data_path=data_dir+'train/', file_names=X_valid, labels=y_valid, transforms=test_transforms)\n",
        "test_data = Dataset_Interpreter(data_path=data_dir+'train/', file_names=X_test, labels=y_test, transforms=test_transforms)\n",
        "\n",
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_iterator = DataLoader(train_data, shuffle=True, batch_size= BATCH_SIZE)\n",
        "valid_iterator = DataLoader(valid_data, batch_size=BATCH_SIZE)\n",
        "test_iterator = DataLoader(test_data, batch_size = BATCH_SIZE)\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "def calculate_accuracy(y_pred, y):\n",
        "    top_pred = y_pred.argmax(1, keepdim = True)\n",
        "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "    acc = correct.float() / y.shape[0]\n",
        "    return acc\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5), #stride=1, padding=0 is a default\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(16*5*5, 120),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(120, 84),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(84, 120)   #num_classes = 120\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        x = self.features(x)\n",
        "        x = x.view(batch_size, -1)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x\n",
        "model = LeNet().to(device)\n",
        "summary(model, (3, 32, 32))\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model= LeNet().to(device)\n",
        "loss_criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer=optim.Adam(model.parameters())\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, device):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for (x, y) in iterator:\n",
        "\n",
        "        x = Variable(torch.FloatTensor(np.array(x))).to(device)\n",
        "        y = Variable(torch.LongTensor(y)).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        y_pred = model(x)\n",
        "\n",
        "        loss = criterion(y_pred, y)\n",
        "\n",
        "        acc = calculate_accuracy(y_pred, y)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion, device):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for (x, y) in iterator:\n",
        "\n",
        "            x = Variable(torch.FloatTensor(np.array(x))).to(device)\n",
        "            y = Variable(torch.LongTensor(y)).to(device)\n",
        "\n",
        "            y_pred = model(x)\n",
        "\n",
        "            loss = criterion(y_pred, y)\n",
        "\n",
        "            acc = calculate_accuracy(y_pred, y)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def fit_model(model, model_name, train_iterator, valid_iterator, optimizer, loss_criterion, device, epochs):\n",
        "    \"\"\" Fits a dataset to model\"\"\"\n",
        "    best_valid_loss = float('inf')\n",
        "\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    train_accs = []\n",
        "    valid_accs = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        train_loss, train_acc = train(model, train_iterator, optimizer, loss_criterion, device)\n",
        "        valid_loss, valid_acc = evaluate(model, valid_iterator, loss_criterion, device)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        valid_losses.append(valid_loss)\n",
        "        train_accs.append(train_acc*100)\n",
        "        valid_accs.append(valid_acc*100)\n",
        "\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), f'{model_name}.pt')\n",
        "\n",
        "        end_time = time.time()\n",
        "\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "\n",
        "    return pd.DataFrame({f'{model_name}_Training_Loss':train_losses,\n",
        "                        f'{model_name}_Training_Acc':train_accs,\n",
        "                        f'{model_name}_Validation_Loss':valid_losses,\n",
        "                        f'{model_name}_Validation_Acc':valid_accs})\n",
        "\n",
        "train_stats_LeNet = fit_model(model, 'LeNet', train_iterator, valid_iterator, optimizer, loss_criterion, device, epochs=20)\n",
        "\n",
        "def plot_training_statistics(train_stats, model_name):\n",
        "\n",
        "    fig, axes = plt.subplots(2, figsize=(15,15))\n",
        "    axes[0].plot(train_stats[f'{model_name}_Training_Loss'], label=f'{model_name}_Training_Loss')\n",
        "    axes[0].plot(train_stats[f'{model_name}_Validation_Loss'], label=f'{model_name}_Validation_Loss')\n",
        "    axes[1].plot(train_stats[f'{model_name}_Training_Acc'], label=f'{model_name}_Training_Acc')\n",
        "    axes[1].plot(train_stats[f'{model_name}_Validation_Acc'], label=f'{model_name}_Validation_Acc')\n",
        "\n",
        "    axes[0].set_xlabel(\"Number of Epochs\"), axes[0].set_ylabel(\"Loss\")\n",
        "    axes[1].set_xlabel(\"Number of Epochs\"), axes[1].set_ylabel(\"Accuracy in %\")\n",
        "\n",
        "    axes[0].legend(), axes[1].legend()\n",
        "\n",
        "plot_training_statistics(train_stats_LeNet, 'LeNet')\n",
        "\n",
        "\n",
        "###### TRAINING ON RESNET-18 WITH TRANSFER LEARNING ######\n",
        "from torchvision import models\n",
        "model = models.resnet18(pretrained=True).to(device)\n",
        "print(model)\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if(\"bn\" not in name):\n",
        "        param.requires_grad = False\n",
        "\n",
        "model.fc = nn.Linear(model.fc.in_features,120).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr = 1e-2)\n",
        "\n",
        "normalize = transforms.Normalize(\n",
        "   mean=[0.485, 0.456, 0.406],\n",
        "   std=[0.229, 0.224, 0.225]\n",
        ")\n",
        "transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)\n",
        "train_transforms = transforms.Compose([transforms.Resize(224),\n",
        "                               transforms.CenterCrop(224),\n",
        "                               transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0),\n",
        "                               transforms.RandomHorizontalFlip(p=0.5),\n",
        "                               transforms.RandomVerticalFlip(p=0.5),\n",
        "                               transforms.RandomGrayscale(p=0.1),\n",
        "                               transforms.ToTensor(),\n",
        "                               normalize])\n",
        "test_transforms = transforms.Compose([transforms.Resize(224),\n",
        "                               transforms.CenterCrop(224),\n",
        "                               transforms.ToTensor(),\n",
        "                               normalize])\n",
        "\n",
        "train_data = Dataset_Interpreter(data_path=data_dir+'train/', file_names=X_train, labels=y_train, transforms=train_transforms)\n",
        "valid_data = Dataset_Interpreter(data_path=data_dir+'train/', file_names=X_valid, labels=y_valid, transforms=test_transforms)\n",
        "test_data = Dataset_Interpreter(data_path=data_dir+'train/', file_names=X_test, labels=y_test, transforms=test_transforms)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_iterator = DataLoader(train_data, shuffle=True, batch_size= BATCH_SIZE)\n",
        "valid_iterator = DataLoader(valid_data, batch_size=BATCH_SIZE)\n",
        "test_iterator = DataLoader(test_data, batch_size = BATCH_SIZE)\n",
        "\n",
        "train_stats_ResNet18 = fit_model(model, 'ResNet18', train_iterator, valid_iterator, optimizer, loss_criterion, device, epochs=20)\n",
        "\n",
        "plot_training_statistics(train_stats_ResNet18, 'ResNet18')\n",
        "\n",
        "\n",
        "#### TRAINING ON RESNET-34 WITH TRANSFER LEARNING ####\n",
        "from torchvision import models\n",
        "model = models.resnet34(pretrained=True).to(device)\n",
        "print(model)\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if(\"bn\" not in name):\n",
        "        param.requires_grad = False\n",
        "\n",
        "model.fc = nn.Linear(model.fc.in_features,120).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr = 1e-2)\n",
        "\n",
        "normalize = transforms.Normalize(\n",
        "   mean=[0.485, 0.456, 0.406],\n",
        "   std=[0.229, 0.224, 0.225]\n",
        ")\n",
        "transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)\n",
        "train_transforms = transforms.Compose([transforms.Resize(224),\n",
        "                               transforms.CenterCrop(224),\n",
        "                               transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0),\n",
        "                               transforms.RandomHorizontalFlip(p=0.5),\n",
        "                               transforms.RandomVerticalFlip(p=0.5),\n",
        "                               transforms.RandomGrayscale(p=0.1),\n",
        "                               transforms.ToTensor(),\n",
        "                               normalize])\n",
        "test_transforms = transforms.Compose([transforms.Resize(224),\n",
        "                               transforms.CenterCrop(224),\n",
        "                               transforms.ToTensor(),\n",
        "                               normalize])\n",
        "\n",
        "train_data = Dataset_Interpreter(data_path=data_dir+'train/', file_names=X_train, labels=y_train, transforms=train_transforms)\n",
        "valid_data = Dataset_Interpreter(data_path=data_dir+'train/', file_names=X_valid, labels=y_valid, transforms=test_transforms)\n",
        "test_data = Dataset_Interpreter(data_path=data_dir+'train/', file_names=X_test, labels=y_test, transforms=test_transforms)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_iterator = DataLoader(train_data, shuffle=True, batch_size= BATCH_SIZE)\n",
        "valid_iterator = DataLoader(valid_data, batch_size=BATCH_SIZE)\n",
        "test_iterator = DataLoader(test_data, batch_size = BATCH_SIZE)\n",
        "\n",
        "train_stats_ResNet34 = fit_model(model, 'ResNet34', train_iterator, valid_iterator, optimizer, loss_criterion, device, epochs=20)"
      ]
    }
  ]
}