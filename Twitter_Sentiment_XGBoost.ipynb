{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdpxV1fpQenVXiEIQGneo8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/canerskrc/Deep_Learning_Project/blob/main/Twitter_Sentiment_XGBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCbSLi1X-EeQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.options.mode.chained_assignment = None\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('./twitter_data'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import re\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "val = pd.read_csv('./twitter_data/twitter_validation.csv')\n",
        "train = pd.read_csv('./twitter_data/twitter_training.csv')\n",
        "\n",
        "train.columns = ['id', 'information', 'type', 'text']\n",
        "print(train.head())\n",
        "\n",
        "val.columns=['id','information','type','text']\n",
        "val.head()\n",
        "\n",
        "train_data=train\n",
        "train_data\n",
        "\n",
        "val_data=val\n",
        "val_data\n",
        "\n",
        "#Text transformation\n",
        "train_data[\"lower\"]=train_data.text.str.lower() #lowercase\n",
        "train_data[\"lower\"]=[str(data) for data in train_data.lower] #converting all to string\n",
        "train_data[\"lower\"]=train_data.lower.apply(lambda x: re.sub('[^A-Za-z0-9 ]+', ' ', x)) #regex\n",
        "val_data[\"lower\"]=val_data.text.str.lower() #lowercase\n",
        "val_data[\"lower\"]=[str(data) for data in val_data.lower] #converting all to string\n",
        "val_data[\"lower\"]=val_data.lower.apply(lambda x: re.sub('[^A-Za-z0-9 ]+', ' ', x)) #regex\n",
        "\n",
        "train_data.head()\n",
        "\n",
        "word_cloud_text = ''.join(train_data[train_data[\"type\"]==\"Positive\"].lower)\n",
        "#Creation of wordcloud\n",
        "wordcloud = WordCloud(\n",
        "    max_font_size=100,\n",
        "    max_words=100,\n",
        "    background_color=\"black\",\n",
        "    scale=10,\n",
        "    width=800,\n",
        "    height=800\n",
        ").generate(word_cloud_text)\n",
        "#Figure properties\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "word_cloud_text = ''.join(train_data[train_data[\"type\"]==\"Negative\"].lower)\n",
        "#Creation of wordcloud\n",
        "wordcloud = WordCloud(\n",
        "    max_font_size=100,\n",
        "    max_words=100,\n",
        "    background_color=\"black\",\n",
        "    scale=10,\n",
        "    width=800,\n",
        "    height=800\n",
        ").generate(word_cloud_text)\n",
        "#Figure properties\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "word_cloud_text = ''.join(train_data[train_data[\"type\"]==\"Irrelevant\"].lower)\n",
        "#Creation of wordcloud\n",
        "wordcloud = WordCloud(\n",
        "    max_font_size=100,\n",
        "    max_words=100,\n",
        "    background_color=\"black\",\n",
        "    scale=10,\n",
        "    width=800,\n",
        "    height=800\n",
        ").generate(word_cloud_text)\n",
        "#Figure properties\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "#Count information per category\n",
        "plot1=train.groupby(by=[\"information\",\"type\"]).count().reset_index()\n",
        "plot1.head()\n",
        "\n",
        "#Figure of comparison per branch\n",
        "plt.figure(figsize=(20,6))\n",
        "sns.barplot(data=plot1,x=\"information\",y=\"id\",hue=\"type\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel(\"Brand\")\n",
        "plt.ylabel(\"Number of tweets\")\n",
        "plt.grid()\n",
        "plt.title(\"Distribution of tweets per Branch and Type\")\n",
        "\n",
        "#### Text Analysis ####\n",
        "#Text splitting\n",
        "tokens_text = [word_tokenize(str(word)) for word in train_data.lower]\n",
        "#Unique word counter\n",
        "tokens_counter = [item for sublist in tokens_text for item in sublist]\n",
        "print(\"Number of tokens: \", len(set(tokens_counter)))\n",
        "\n",
        "print(tokens_text[1])\n",
        "\n",
        "#Choosing english stopwords\n",
        "stopwords_nltk = nltk.corpus.stopwords\n",
        "stop_words = stopwords_nltk.words('english')\n",
        "print(stop_words[:5])\n",
        "\n",
        "#Initial Bag of Words\n",
        "bow_counts = CountVectorizer(\n",
        "    tokenizer=word_tokenize,\n",
        "    stop_words=stop_words, #English Stopwords\n",
        "    ngram_range=(1, 1) #analysis of one word\n",
        ")\n",
        "\n",
        "reviews_train, reviews_test = train_test_split(train_data, test_size=0.2, random_state=0)\n",
        "\n",
        "#Creation of encoding related to train dataset\n",
        "X_train_bow = bow_counts.fit_transform(reviews_train.lower)\n",
        "#Transformation of test dataset with train encoding\n",
        "X_test_bow = bow_counts.transform(reviews_test.lower)\n",
        "print(X_test_bow)\n",
        "y_test_bow.value_counts() / y_test_bow.shape[0]\n",
        "\n",
        "# Logistic regression\n",
        "model1 = LogisticRegression(C=1, solver=\"liblinear\",max_iter=200)\n",
        "model1.fit(X_train_bow, y_train_bow)\n",
        "# Prediction\n",
        "test_pred = model1.predict(X_test_bow)\n",
        "print(\"Accuracy: \", accuracy_score(y_test_bow, test_pred) * 100)\n",
        "\n",
        "#Validation data\n",
        "X_val_bow = bow_counts.transform(val_data.lower)\n",
        "y_val_bow = val_data['type']\n",
        "\n",
        "print(X_val_bow)\n",
        "\n",
        "Val_res = model1.predict(X_val_bow)\n",
        "print(\"Accuracy: \", accuracy_score(y_val_bow, Val_res) * 100)\n",
        "\n",
        "#n-gram of 4 words\n",
        "bow_counts = CountVectorizer(\n",
        "    tokenizer=word_tokenize,\n",
        "    ngram_range=(1,4)\n",
        ")\n",
        "#Data labeling\n",
        "X_train_bow = bow_counts.fit_transform(reviews_train.lower)\n",
        "X_test_bow = bow_counts.transform(reviews_test.lower)\n",
        "X_val_bow = bow_counts.transform(val_data.lower)\n",
        "\n",
        "X_train_bow\n",
        "model2 = LogisticRegression(C=0.9, solver=\"liblinear\",max_iter=1500)\n",
        "# Logistic regression\n",
        "model2.fit(X_train_bow, y_train_bow)\n",
        "# Prediction\n",
        "test_pred_2 = model2.predict(X_test_bow)\n",
        "print(\"Accuracy: \", accuracy_score(y_test_bow, test_pred_2) * 100)\n",
        "\n",
        "y_val_bow = val_data['type']\n",
        "Val_pred_2 = model2.predict(X_val_bow)\n",
        "print(\"Accuracy: \", accuracy_score(y_val_bow, Val_pred_2) * 100)\n",
        "\n",
        "\n",
        "#### XGBoost ####\n",
        "# https://stackoverflow.com/questions/71996617/invalid-classes-inferred-from-unique-values-of-y-expected-0-1-2-3-4-5-got\n",
        "le = LabelEncoder()\n",
        "y_train_bow_num = le.fit_transform(y_train_bow)\n",
        "y_test_bow_num=le.transform(y_test_bow)\n",
        "y_val_bow_num=le.transform(y_val_bow)\n",
        "\n",
        "%%time\n",
        "XGB=XGBClassifier(objective=\"multi:softmax\",n_estimators=1000,colsample_bytree=0.6, subsample=0.6)\n",
        "XGB.fit(X_train_bow, y_train_bow_num)\n",
        "# Prediction\n",
        "test_pred_2 = XGB.predict(X_test_bow)\n",
        "print(\"Accuracy: \", accuracy_score(y_test_bow_num, test_pred_2) * 100)\n",
        "\n",
        "y_val_bow = val_data['type']\n",
        "Val_pred_2 = XGB.predict(X_val_bow)\n",
        "print(\"Accuracy: \", accuracy_score(y_val_bow_num, Val_pred_2) * 100)\n",
        "\n",
        "test_pred_N = XGB.predict(X_train_bow)\n",
        "print(\"Accuracy: \", accuracy_score(y_train_bow_num, test_pred_N) * 100)"
      ]
    }
  ]
}